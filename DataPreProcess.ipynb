{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87bf13f5"
      },
      "source": [
        "#Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffcd612a",
        "outputId": "3d1392d2-b7bf-4aac-c80c-e4b3d7cfd4dc"
      },
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/Pontakorn-Wich/Mini_project/master/data/books_1250_above_reviews.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()\n",
        "\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    book_id                           user_id  \\\n",
            "0  25781157  d37b46b2190ed7c518259f29b47a9b36   \n",
            "1  18774964  d37b46b2190ed7c518259f29b47a9b36   \n",
            "2  12609433  d4b1dcb35db677f20ee45225a5e43be2   \n",
            "3  18774964  b7dbd4518192923079be19c74e049608   \n",
            "4  18774964  ced7b8e0a3340e8af27f2663f442c3bb   \n",
            "\n",
            "                          review_id  rating  \\\n",
            "0  c159507f6f0c4010bcfcda6cee74a817       2   \n",
            "1  3ab2b07073b3bd4134f1a1e8b0053b7f       2   \n",
            "2  e95e9327ce1de178d99aebd20d3f80fb       3   \n",
            "3  e5b3bbef0c8990dda81ff6f4885f62bd       5   \n",
            "4  1da5c13281b915f7aa154d7ea768b654       5   \n",
            "\n",
            "                                         review_text  \\\n",
            "0  This book has gotten many accolades but I foun...   \n",
            "1  This is a story about a cantankerous and sad w...   \n",
            "2  3.5 stars. Interesting, highly readable. I jus...   \n",
            "3                      Loved, loved loved this book.   \n",
            "4  This is possibly the best book I have ever rea...   \n",
            "\n",
            "                       date_added                    date_updated  n_votes  \\\n",
            "0  Fri Jun 03 15:31:38 -0700 2016  Sun Jun 19 07:29:35 -0700 2016       21   \n",
            "1  Fri Sep 11 07:41:34 -0700 2015  Wed Sep 23 08:22:35 -0700 2015       11   \n",
            "2  Tue Jun 05 14:43:15 -0700 2012  Tue Sep 13 10:10:38 -0700 2016        0   \n",
            "3  Mon Dec 19 18:36:41 -0800 2016  Wed Aug 09 11:05:51 -0700 2017        0   \n",
            "4  Sat Jun 18 10:44:15 -0700 2016  Thu Dec 15 17:39:24 -0800 2016        0   \n",
            "\n",
            "   n_comments  \n",
            "0           7  \n",
            "1           6  \n",
            "2           0  \n",
            "3           0  \n",
            "4           0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove non-english reviews"
      ],
      "metadata": {
        "id": "k4BqhqzroVq5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecab755f",
        "outputId": "4cdf6017-44f9-4be9-adc5-390a523c6eeb"
      },
      "source": [
        "get_ipython().system('pip install langdetect')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=c10c038518ac042824dc5f3eaf4a57660df9855e228d7fff546548df60ac59d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "langdetect library installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9446af85"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'langdetect' library has been installed. Now, the next logical step is to use it to detect the language of the reviews in the `review_text` column. This involves importing the necessary function, defining a safe detection function to handle potential errors or empty strings, and applying it to create a new 'language' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "566e5a38",
        "outputId": "18a4137a-71c4-4f28-f4f3-fbbea8b8f21f"
      },
      "source": [
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "def detect_language_safe(text):\n",
        "    if isinstance(text, str) and text.strip():\n",
        "        try:\n",
        "            return detect(text)\n",
        "        except LangDetectException:\n",
        "            return 'unknown'\n",
        "    return 'empty'\n",
        "\n",
        "df['language'] = df['review_text'].apply(detect_language_safe)\n",
        "print(df[['review_text', 'language']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language detection applied to 'review_text' column.\n",
            "                                         review_text language\n",
            "0  This book has gotten many accolades but I foun...       en\n",
            "1  This is a story about a cantankerous and sad w...       en\n",
            "2  3.5 stars. Interesting, highly readable. I jus...       en\n",
            "3                      Loved, loved loved this book.       en\n",
            "4  This is possibly the best book I have ever rea...       en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97fa867c",
        "outputId": "31b125b3-729a-4234-be9d-37072b49afe5"
      },
      "source": [
        "df_english = df[df['language'] == 'en'].copy()\n",
        "print(df_english[['review_text', 'language']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered DataFrame to include only English reviews.\n",
            "                                         review_text language\n",
            "0  This book has gotten many accolades but I foun...       en\n",
            "1  This is a story about a cantankerous and sad w...       en\n",
            "2  3.5 stars. Interesting, highly readable. I jus...       en\n",
            "3                      Loved, loved loved this book.       en\n",
            "4  This is possibly the best book I have ever rea...       en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f2839bc",
        "outputId": "476c2084-21bb-49c0-9ff7-225c942ed788"
      },
      "source": [
        "df = df_english"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame updated to contain only English reviews.\n",
            "                                         review_text language\n",
            "0  This book has gotten many accolades but I foun...       en\n",
            "1  This is a story about a cantankerous and sad w...       en\n",
            "2  3.5 stars. Interesting, highly readable. I jus...       en\n",
            "3                      Loved, loved loved this book.       en\n",
            "4  This is possibly the best book I have ever rea...       en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b6ac33d"
      },
      "source": [
        "##Special Characters Removal\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56f71b79",
        "outputId": "7a37ebca-bd83-4157-f490-0c7da0c6a238"
      },
      "source": [
        "import re\n",
        "def remove_noise(text):\n",
        "    # Remove characters that are not letters or spaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_review_text'] = df['review_text'].apply(remove_noise)\n",
        "print(df[['review_text', 'cleaned_review_text']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noise removal applied to 'review_text' column.\n",
            "                                         review_text  \\\n",
            "0  This book has gotten many accolades but I foun...   \n",
            "1  This is a story about a cantankerous and sad w...   \n",
            "2  3.5 stars. Interesting, highly readable. I jus...   \n",
            "3                      Loved, loved loved this book.   \n",
            "4  This is possibly the best book I have ever rea...   \n",
            "\n",
            "                                 cleaned_review_text  \n",
            "0  This book has gotten many accolades but I foun...  \n",
            "1  This is a story about a cantankerous and sad w...  \n",
            "2  stars Interesting highly readable I just felt ...  \n",
            "3                        Loved loved loved this book  \n",
            "4  This is possibly the best book I have ever rea...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a2062d1"
      },
      "source": [
        "##Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a3c219e",
        "outputId": "eae72552-4e96-4720-de0f-4839dddd6c84"
      },
      "source": [
        "df['normalized_text'] = df['cleaned_review_text'].str.lower()\n",
        "df['normalized_text'] = df['normalized_text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "print(\"Text normalized (lowercase and whitespace removal).\")\n",
        "print(df[['cleaned_review_text', 'normalized_text']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text normalized (lowercase and whitespace removal).\n",
            "                                 cleaned_review_text  \\\n",
            "0  This book has gotten many accolades but I foun...   \n",
            "1  This is a story about a cantankerous and sad w...   \n",
            "2  stars Interesting highly readable I just felt ...   \n",
            "3                        Loved loved loved this book   \n",
            "4  This is possibly the best book I have ever rea...   \n",
            "\n",
            "                                     normalized_text  \n",
            "0  this book has gotten many accolades but i foun...  \n",
            "1  this is a story about a cantankerous and sad w...  \n",
            "2  stars interesting highly readable i just felt ...  \n",
            "3                        loved loved loved this book  \n",
            "4  this is possibly the best book i have ever rea...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22042904"
      },
      "source": [
        "##Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5481cb3",
        "outputId": "d628b3c4-bbe9-4f48-f8e5-7787a098726c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'stopwords' corpus downloaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88da6f58",
        "outputId": "735a2cd0-65fa-4ac5-d6ee-d473f942f33a"
      },
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'punkt_tab' resource downloaded successfully as per error message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab38ad7e",
        "outputId": "6da03a65-b5a8-40de-8b07-d2c20305f577"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "df['normalized_text_no_stopwords'] = df['normalized_text'].apply(remove_stopwords)\n",
        "print(df[['normalized_text', 'normalized_text_no_stopwords']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords removed from normalized_text' column.\n",
            "                                     normalized_text  \\\n",
            "0  this book has gotten many accolades but i foun...   \n",
            "1  this is a story about a cantankerous and sad w...   \n",
            "2  stars interesting highly readable i just felt ...   \n",
            "3                        loved loved loved this book   \n",
            "4  this is possibly the best book i have ever rea...   \n",
            "\n",
            "                        normalized_text_no_stopwords  \n",
            "0  book gotten many accolades found snooze fest f...  \n",
            "1  story cantankerous sad widower lost live boist...  \n",
            "2  stars interesting highly readable felt example...  \n",
            "3                             loved loved loved book  \n",
            "4  possibly best book ever read giggling first pa...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9237b67c"
      },
      "source": [
        "##Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bef626d",
        "outputId": "5239641c-3e1a-44e3-bc7b-2343b8201bee"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "df['tokenized_text'] = df['normalized_text_no_stopwords'].apply(tokenize_text)\n",
        "\n",
        "print(df[['normalized_text_no_stopwords', 'tokenized_text']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text tokenization applied to 'normalized_text' column.\n",
            "                        normalized_text_no_stopwords  \\\n",
            "0  book gotten many accolades found snooze fest f...   \n",
            "1  story cantankerous sad widower lost live boist...   \n",
            "2  stars interesting highly readable felt example...   \n",
            "3                             loved loved loved book   \n",
            "4  possibly best book ever read giggling first pa...   \n",
            "\n",
            "                                      tokenized_text  \n",
            "0  [book, gotten, many, accolades, found, snooze,...  \n",
            "1  [story, cantankerous, sad, widower, lost, live...  \n",
            "2  [stars, interesting, highly, readable, felt, e...  \n",
            "3                        [loved, loved, loved, book]  \n",
            "4  [possibly, best, book, ever, read, giggling, f...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000170be",
        "outputId": "cf3f1120-42f4-4196-af0d-e4eac11facc0"
      },
      "source": [
        "get_ipython().system('pip install spacy')\n",
        "print(\"SpaCy library installed successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "SpaCy library installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec1705ee"
      },
      "source": [
        "**Reasoning**:\n",
        "The SpaCy library has been installed. The next step, as per the subtask instructions, is to download a suitable English language model (e.g., `en_core_web_sm`) using `spacy.cli.download`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d1401bf",
        "outputId": "7ac497db-8924-4c6b-82de-b5e718b9c416"
      },
      "source": [
        "import spacy\n",
        "\n",
        "try:\n",
        "    spacy.load('en_core_web_sm')\n",
        "    print(\"'en_core_web_sm' model already downloaded.\")\n",
        "except OSError:\n",
        "    print(\"Downloading 'en_core_web_sm' model...\")\n",
        "    spacy.cli.download('en_core_web_sm')\n",
        "    print(\"'en_core_web_sm' model downloaded successfully.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'en_core_web_sm' model already downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dce9469"
      },
      "source": [
        "##Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0efb1ce",
        "outputId": "7fad4eaa-93d1-4f39-868c-0061a3b474bd"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    # Extract lemmas for each token and join them into a string\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "df['lemmatized_text'] = df['normalized_text'].apply(lemmatize_text)\n",
        "print(df[['normalized_text', 'lemmatized_text']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization applied to 'normalized_text' column.\n",
            "                                     normalized_text  \\\n",
            "0  this book has gotten many accolades but i foun...   \n",
            "1  this is a story about a cantankerous and sad w...   \n",
            "2  stars interesting highly readable i just felt ...   \n",
            "3                        loved loved loved this book   \n",
            "4  this is possibly the best book i have ever rea...   \n",
            "\n",
            "                                     lemmatized_text  \n",
            "0  this book have get many accolade but I find it...  \n",
            "1  this be a story about a cantankerous and sad w...  \n",
            "2  star interesting highly readable I just feel s...  \n",
            "3                           love love love this book  \n",
            "4  this be possibly the good book I have ever rea...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad0ec5c5"
      },
      "source": [
        "##POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0408242d",
        "outputId": "d34478f7-9a11-4012-def9-525ad22dcf88"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy library imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69954aed",
        "outputId": "89a71262-bae6-4373-cc5e-b7a84be5ff07"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def pos_tag_text(text):\n",
        "    doc = nlp(text)\n",
        "    # Extract POS tag for each token and join them into a string\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "    return pos_tags\n",
        "\n",
        "df['pos_tagged_text'] = df['lemmatized_text'].apply(pos_tag_text)\n",
        "\n",
        "print(df[['lemmatized_text', 'pos_tagged_text']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging applied to 'lemmatized_text' column.\n",
            "                                     lemmatized_text  \\\n",
            "0  this book have get many accolade but I find it...   \n",
            "1  this be a story about a cantankerous and sad w...   \n",
            "2  star interesting highly readable I just feel s...   \n",
            "3                           love love love this book   \n",
            "4  this be possibly the good book I have ever rea...   \n",
            "\n",
            "                                     pos_tagged_text  \n",
            "0  [(this, DET), (book, NOUN), (have, AUX), (get,...  \n",
            "1  [(this, PRON), (be, AUX), (a, DET), (story, NO...  \n",
            "2  [(star, PROPN), (interesting, ADJ), (highly, A...  \n",
            "3  [(love, NOUN), (love, NOUN), (love, VERB), (th...  \n",
            "4  [(this, PRON), (be, AUX), (possibly, ADV), (th...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ec5a48"
      },
      "source": [
        "##Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "882b2263",
        "outputId": "f2c48cf6-33c9-42ef-ca17-4c27a27d5a27"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    # Extract named entities\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "df['named_entities'] = df['lemmatized_text'].apply(extract_named_entities)\n",
        "print(df[['lemmatized_text', 'named_entities']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity Recognition applied to 'lemmatized_text' column.\n",
            "                                     lemmatized_text      named_entities\n",
            "0  this book have get many accolade but I find it...  [(four, CARDINAL)]\n",
            "1  this be a story about a cantankerous and sad w...                  []\n",
            "2  star interesting highly readable I just feel s...                  []\n",
            "3                           love love love this book                  []\n",
            "4  this be possibly the good book I have ever rea...  [(first, ORDINAL)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30235d29",
        "outputId": "82551ff7-d7cb-42a3-be6b-7fed5330b7fc"
      },
      "source": [
        "print(df[['review_text', 'cleaned_review_text', 'normalized_text_no_stopwords', 'normalized_text', 'lemmatized_text', 'pos_tagged_text', 'named_entities']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample of processed data showing evolution through stages:\n",
            "                                         review_text  \\\n",
            "0  This book has gotten many accolades but I foun...   \n",
            "1  This is a story about a cantankerous and sad w...   \n",
            "2  3.5 stars. Interesting, highly readable. I jus...   \n",
            "3                      Loved, loved loved this book.   \n",
            "4  This is possibly the best book I have ever rea...   \n",
            "\n",
            "                                 cleaned_review_text  \\\n",
            "0  This book has gotten many accolades but I foun...   \n",
            "1  This is a story about a cantankerous and sad w...   \n",
            "2  stars Interesting highly readable I just felt ...   \n",
            "3                        Loved loved loved this book   \n",
            "4  This is possibly the best book I have ever rea...   \n",
            "\n",
            "                        normalized_text_no_stopwords  \\\n",
            "0  book gotten many accolades found snooze fest f...   \n",
            "1  story cantankerous sad widower lost live boist...   \n",
            "2  stars interesting highly readable felt example...   \n",
            "3                             loved loved loved book   \n",
            "4  possibly best book ever read giggling first pa...   \n",
            "\n",
            "                                     normalized_text  \\\n",
            "0  this book has gotten many accolades but i foun...   \n",
            "1  this is a story about a cantankerous and sad w...   \n",
            "2  stars interesting highly readable i just felt ...   \n",
            "3                        loved loved loved this book   \n",
            "4  this is possibly the best book i have ever rea...   \n",
            "\n",
            "                                     lemmatized_text  \\\n",
            "0  this book have get many accolade but I find it...   \n",
            "1  this be a story about a cantankerous and sad w...   \n",
            "2  star interesting highly readable I just feel s...   \n",
            "3                           love love love this book   \n",
            "4  this be possibly the good book I have ever rea...   \n",
            "\n",
            "                                     pos_tagged_text      named_entities  \n",
            "0  [(this, DET), (book, NOUN), (have, AUX), (get,...  [(four, CARDINAL)]  \n",
            "1  [(this, PRON), (be, AUX), (a, DET), (story, NO...                  []  \n",
            "2  [(star, PROPN), (interesting, ADJ), (highly, A...                  []  \n",
            "3  [(love, NOUN), (love, NOUN), (love, VERB), (th...                  []  \n",
            "4  [(this, PRON), (be, AUX), (possibly, ADV), (th...  [(first, ORDINAL)]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32dab1c3",
        "outputId": "8cfc662b-1084-4fcd-a8b0-b36efb16242a"
      },
      "source": [
        "# # Sort the DataFrame by the length of the 'review_text' column\n",
        "# df_sorted_by_review_length = df.copy()\n",
        "# df_sorted_by_review_length['review_text_length'] = df_sorted_by_review_length['review_text'].apply(len)\n",
        "# df_sorted_by_review_length = df_sorted_by_review_length.sort_values(by='review_text_length', ascending=True)\n",
        "# print(df_sorted_by_review_length[['review_text', 'cleaned_review_text', 'normalized_text_no_stopwords', 'normalized_text', 'lemmatized_text', 'pos_tagged_text', 'named_entities']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample of processed data sorted by 'review_text' length (shortest to longest):\n",
            "      review_text cleaned_review_text normalized_text_no_stopwords  \\\n",
            "10557           T                   T                                \n",
            "8160            I                   I                                \n",
            "1512            i                   i                                \n",
            "8178            .                                                    \n",
            "8650            2                                                    \n",
            "\n",
            "      normalized_text lemmatized_text pos_tagged_text named_entities  \n",
            "10557               t               t    [(t, PROPN)]             []  \n",
            "8160                i               I     [(I, PRON)]             []  \n",
            "1512                i               I     [(I, PRON)]             []  \n",
            "8178                                               []             []  \n",
            "8650                                               []             []  \n"
          ]
        }
      ]
    }
  ]
}
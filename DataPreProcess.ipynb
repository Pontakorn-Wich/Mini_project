{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87bf13f5"
      },
      "source": [
        "#Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffcd612a",
        "outputId": "3d1392d2-b7bf-4aac-c80c-e4b3d7cfd4dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    book_id                           user_id  \\\n",
            "0  13526165  8842281e1d1347389f2ab93d60773d4d   \n",
            "1   9938498  8842281e1d1347389f2ab93d60773d4d   \n",
            "2   2767052  8842281e1d1347389f2ab93d60773d4d   \n",
            "3    136251  8842281e1d1347389f2ab93d60773d4d   \n",
            "4  15507958  7504b2aee1ecb5b2872d3da381c6c91e   \n",
            "\n",
            "                          review_id  rating  \\\n",
            "0  51fe3e46c7f8eb39f5623d1bd8bbbbfc       5   \n",
            "1  bff5654c639c7b008571c3d4398d930a       4   \n",
            "2  248c011811e945eca861b5c31a549291       5   \n",
            "3  132eab4c9a3724493204cc083e0e2ecc       5   \n",
            "4  63ff74279e46b247cb1754313b160006       4   \n",
            "\n",
            "                                         review_text  \\\n",
            "0  My wife suggested I read this book, and I resi...   \n",
            "1  Great story of the US Ambassador to Germany an...   \n",
            "2  I cracked and finally picked this up. Very enj...   \n",
            "3  Loved every minute. So sad there isn't another...   \n",
            "4  I finished reading this days ago and cant get ...   \n",
            "\n",
            "                       date_added                    date_updated  n_votes  \\\n",
            "0  Thu May 30 20:25:19 -0700 2013  Wed Mar 22 11:47:21 -0700 2017       23   \n",
            "1  Sat May 21 15:04:50 -0700 2011  Wed Mar 22 11:46:56 -0700 2017        5   \n",
            "2  Wed Jan 13 13:38:25 -0800 2010  Wed Mar 22 11:46:36 -0700 2017       24   \n",
            "3  Mon Dec 22 10:38:27 -0800 2008  Wed Mar 22 11:46:18 -0700 2017        6   \n",
            "4  Wed Sep 10 19:33:44 -0700 2014  Sun Sep 21 19:03:38 -0700 2014        0   \n",
            "\n",
            "   n_comments  \n",
            "0           5  \n",
            "1           1  \n",
            "2          25  \n",
            "3           3  \n",
            "4           0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/Pontakorn-Wich/Mini_project/master/data/books_1250_above_reviews.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4BqhqzroVq5"
      },
      "source": [
        "## Remove non-english reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecab755f",
        "outputId": "4cdf6017-44f9-4be9-adc5-390a523c6eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
            "     ---------------------------------------- 981.5/981.5 kB 13.1 MB/s  0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: six in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (pyproject.toml): started\n",
            "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993363 sha256=6b65427445f2ccab90dd71fd5a81b1dba3b81eff5a8fa4ba083dbb98e2e89dad\n",
            "  Stored in directory: c:\\users\\kenpo\\appdata\\local\\pip\\cache\\wheels\\eb\\87\\25\\2dddf1c94e1786054e25022ec5530bfed52bad86d882999c48\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: C:\\Users\\kenpo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "get_ipython().system('pip install langdetect')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9446af85"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'langdetect' library has been installed. Now, the next logical step is to use it to detect the language of the reviews in the `review_text` column. This involves importing the necessary function, defining a safe detection function to handle potential errors or empty strings, and applying it to create a new 'language' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "566e5a38",
        "outputId": "18a4137a-71c4-4f28-f4f3-fbbea8b8f21f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         review_text language\n",
            "0  My wife suggested I read this book, and I resi...       en\n",
            "1  Great story of the US Ambassador to Germany an...       en\n",
            "2  I cracked and finally picked this up. Very enj...       en\n",
            "3  Loved every minute. So sad there isn't another...       en\n",
            "4  I finished reading this days ago and cant get ...       en\n"
          ]
        }
      ],
      "source": [
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "def detect_language_safe(text):\n",
        "    if isinstance(text, str) and text.strip():\n",
        "        try:\n",
        "            return detect(text)\n",
        "        except LangDetectException:\n",
        "            return 'unknown'\n",
        "    return 'empty'\n",
        "\n",
        "df['language'] = df['review_text'].apply(detect_language_safe)\n",
        "print(df[['review_text', 'language']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97fa867c",
        "outputId": "31b125b3-729a-4234-be9d-37072b49afe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         review_text language\n",
            "0  My wife suggested I read this book, and I resi...       en\n",
            "1  Great story of the US Ambassador to Germany an...       en\n",
            "2  I cracked and finally picked this up. Very enj...       en\n",
            "3  Loved every minute. So sad there isn't another...       en\n",
            "4  I finished reading this days ago and cant get ...       en\n"
          ]
        }
      ],
      "source": [
        "df_english = df[df['language'] == 'en'].copy()\n",
        "print(df_english[['review_text', 'language']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f2839bc",
        "outputId": "476c2084-21bb-49c0-9ff7-225c942ed788"
      },
      "outputs": [],
      "source": [
        "df = df_english"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b6ac33d"
      },
      "source": [
        "##Special Characters Removal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56f71b79",
        "outputId": "7a37ebca-bd83-4157-f490-0c7da0c6a238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         review_text  \\\n",
            "0  My wife suggested I read this book, and I resi...   \n",
            "1  Great story of the US Ambassador to Germany an...   \n",
            "2  I cracked and finally picked this up. Very enj...   \n",
            "3  Loved every minute. So sad there isn't another...   \n",
            "4  I finished reading this days ago and cant get ...   \n",
            "\n",
            "                                 cleaned_review_text  \n",
            "0  My wife suggested I read this book and I resis...  \n",
            "1  Great story of the US Ambassador to Germany an...  \n",
            "2  I cracked and finally picked this up Very enjo...  \n",
            "3  Loved every minute So sad there isnt another I...  \n",
            "4  I finished reading this days ago and cant get ...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def remove_noise(text):\n",
        "    # Remove characters that are not letters or spaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_review_text'] = df['review_text'].apply(remove_noise)\n",
        "print(df[['review_text', 'cleaned_review_text']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a2062d1"
      },
      "source": [
        "##Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a3c219e",
        "outputId": "eae72552-4e96-4720-de0f-4839dddd6c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text normalized (lowercase and whitespace removal).\n",
            "                                 cleaned_review_text  \\\n",
            "0  My wife suggested I read this book and I resis...   \n",
            "1  Great story of the US Ambassador to Germany an...   \n",
            "2  I cracked and finally picked this up Very enjo...   \n",
            "3  Loved every minute So sad there isnt another I...   \n",
            "4  I finished reading this days ago and cant get ...   \n",
            "\n",
            "                                     normalized_text  \n",
            "0  my wife suggested i read this book and i resis...  \n",
            "1  great story of the us ambassador to germany an...  \n",
            "2  i cracked and finally picked this up very enjo...  \n",
            "3  loved every minute so sad there isnt another i...  \n",
            "4  i finished reading this days ago and cant get ...  \n"
          ]
        }
      ],
      "source": [
        "df['normalized_text'] = df['cleaned_review_text'].str.lower()\n",
        "df['normalized_text'] = df['normalized_text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "print(\"Text normalized (lowercase and whitespace removal).\")\n",
        "print(df[['cleaned_review_text', 'normalized_text']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22042904"
      },
      "source": [
        "##Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5481cb3",
        "outputId": "d628b3c4-bbe9-4f48-f8e5-7787a098726c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kenpo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88da6f58",
        "outputId": "735a2cd0-65fa-4ac5-d6ee-d473f942f33a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\kenpo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab38ad7e",
        "outputId": "6da03a65-b5a8-40de-8b07-d2c20305f577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     normalized_text  \\\n",
            "0  my wife suggested i read this book and i resis...   \n",
            "1  great story of the us ambassador to germany an...   \n",
            "2  i cracked and finally picked this up very enjo...   \n",
            "3  loved every minute so sad there isnt another i...   \n",
            "4  i finished reading this days ago and cant get ...   \n",
            "\n",
            "                        normalized_text_no_stopwords  \n",
            "0  wife suggested read book resisted impression s...  \n",
            "1  great story us ambassador germany experience b...  \n",
            "2  cracked finally picked enjoyable quick read co...  \n",
            "3  loved every minute sad isnt another thought jk...  \n",
            "4  finished reading days ago cant get head heart ...  \n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "df['normalized_text_no_stopwords'] = df['normalized_text'].apply(remove_stopwords)\n",
        "print(df[['normalized_text', 'normalized_text_no_stopwords']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9237b67c"
      },
      "source": [
        "##Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bef626d",
        "outputId": "5239641c-3e1a-44e3-bc7b-2343b8201bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        normalized_text_no_stopwords  \\\n",
            "0  wife suggested read book resisted impression s...   \n",
            "1  great story us ambassador germany experience b...   \n",
            "2  cracked finally picked enjoyable quick read co...   \n",
            "3  loved every minute sad isnt another thought jk...   \n",
            "4  finished reading days ago cant get head heart ...   \n",
            "\n",
            "                                      tokenized_text  \n",
            "0  [wife, suggested, read, book, resisted, impres...  \n",
            "1  [great, story, us, ambassador, germany, experi...  \n",
            "2  [cracked, finally, picked, enjoyable, quick, r...  \n",
            "3  [loved, every, minute, sad, isnt, another, tho...  \n",
            "4  [finished, reading, days, ago, cant, get, head...  \n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "df['tokenized_text'] = df['normalized_text_no_stopwords'].apply(tokenize_text)\n",
        "\n",
        "print(df[['normalized_text_no_stopwords', 'tokenized_text']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000170be",
        "outputId": "cf3f1120-42f4-4196-af0d-e4eac11facc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: spacy in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.8.10)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (8.3.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (77.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->spacy) (3.0.3)\n",
            "SpaCy library installed successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: C:\\Users\\kenpo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "get_ipython().system('pip install spacy')\n",
        "print(\"SpaCy library installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec1705ee"
      },
      "source": [
        "**Reasoning**:\n",
        "The SpaCy library has been installed. The next step, as per the subtask instructions, is to download a suitable English language model (e.g., `en_core_web_sm`) using `spacy.cli.download`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d1401bf",
        "outputId": "7ac497db-8924-4c6b-82de-b5e718b9c416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'en_core_web_sm' model already downloaded.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "try:\n",
        "    spacy.load('en_core_web_sm')\n",
        "    print(\"'en_core_web_sm' model already downloaded.\")\n",
        "except OSError:\n",
        "    print(\"Downloading 'en_core_web_sm' model...\")\n",
        "    spacy.cli.download('en_core_web_sm')\n",
        "    print(\"'en_core_web_sm' model downloaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dce9469"
      },
      "source": [
        "##Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0efb1ce",
        "outputId": "7fad4eaa-93d1-4f39-868c-0061a3b474bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     normalized_text  \\\n",
            "0  my wife suggested i read this book and i resis...   \n",
            "1  great story of the us ambassador to germany an...   \n",
            "2  i cracked and finally picked this up very enjo...   \n",
            "3  loved every minute so sad there isnt another i...   \n",
            "4  i finished reading this days ago and cant get ...   \n",
            "\n",
            "                                     lemmatized_text  \n",
            "0  my wife suggest I read this book and I resist ...  \n",
            "1  great story of the us ambassador to germany an...  \n",
            "2  I crack and finally pick this up very enjoyabl...  \n",
            "3  love every minute so sad there be not another ...  \n",
            "4  I finish read this day ago and can not get thi...  \n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    # Extract lemmas for each token and join them into a string\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "df['lemmatized_text'] = df['normalized_text'].apply(lemmatize_text)\n",
        "print(df[['normalized_text', 'lemmatized_text']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad0ec5c5"
      },
      "source": [
        "##POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0408242d",
        "outputId": "d34478f7-9a11-4012-def9-525ad22dcf88"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69954aed",
        "outputId": "89a71262-bae6-4373-cc5e-b7a84be5ff07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     lemmatized_text  \\\n",
            "0  my wife suggest I read this book and I resist ...   \n",
            "1  great story of the us ambassador to germany an...   \n",
            "2  I crack and finally pick this up very enjoyabl...   \n",
            "3  love every minute so sad there be not another ...   \n",
            "4  I finish read this day ago and can not get thi...   \n",
            "\n",
            "                                     pos_tagged_text  \n",
            "0  [(my, PRON), (wife, NOUN), (suggest, VERB), (I...  \n",
            "1  [(great, ADJ), (story, NOUN), (of, ADP), (the,...  \n",
            "2  [(I, PRON), (crack, VERB), (and, CCONJ), (fina...  \n",
            "3  [(love, NOUN), (every, DET), (minute, NOUN), (...  \n",
            "4  [(I, PRON), (finish, VERB), (read, VERB), (thi...  \n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def pos_tag_text(text):\n",
        "    doc = nlp(text)\n",
        "    # Extract POS tag for each token and join them into a string\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "    return pos_tags\n",
        "\n",
        "df['pos_tagged_text'] = df['lemmatized_text'].apply(pos_tag_text)\n",
        "\n",
        "print(df[['lemmatized_text', 'pos_tagged_text']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ec5a48"
      },
      "source": [
        "##Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "882b2263",
        "outputId": "f2c48cf6-33c9-42ef-ca17-4c27a27d5a27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     lemmatized_text  \\\n",
            "0  my wife suggest I read this book and I resist ...   \n",
            "1  great story of the us ambassador to germany an...   \n",
            "2  I crack and finally pick this up very enjoyabl...   \n",
            "3  love every minute so sad there be not another ...   \n",
            "4  I finish read this day ago and can not get thi...   \n",
            "\n",
            "                                      named_entities  \n",
            "0          [(la, GPE), (seattle, GPE), (india, GPE)]  \n",
            "1  [(us, GPE), (germany, GPE), (martha, PERSON), ...  \n",
            "2                                [(each year, DATE)]  \n",
            "3  [(every minute, TIME), (a month later, DATE), ...  \n",
            "4                             [(this day ago, DATE)]  \n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    # Extract named entities\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "df['named_entities'] = df['lemmatized_text'].apply(extract_named_entities)\n",
        "print(df[['lemmatized_text', 'named_entities']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30235d29",
        "outputId": "82551ff7-d7cb-42a3-be6b-7fed5330b7fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         review_text  \\\n",
            "0  My wife suggested I read this book, and I resi...   \n",
            "1  Great story of the US Ambassador to Germany an...   \n",
            "2  I cracked and finally picked this up. Very enj...   \n",
            "3  Loved every minute. So sad there isn't another...   \n",
            "4  I finished reading this days ago and cant get ...   \n",
            "\n",
            "                                 cleaned_review_text  \\\n",
            "0  My wife suggested I read this book and I resis...   \n",
            "1  Great story of the US Ambassador to Germany an...   \n",
            "2  I cracked and finally picked this up Very enjo...   \n",
            "3  Loved every minute So sad there isnt another I...   \n",
            "4  I finished reading this days ago and cant get ...   \n",
            "\n",
            "                        normalized_text_no_stopwords  \\\n",
            "0  wife suggested read book resisted impression s...   \n",
            "1  great story us ambassador germany experience b...   \n",
            "2  cracked finally picked enjoyable quick read co...   \n",
            "3  loved every minute sad isnt another thought jk...   \n",
            "4  finished reading days ago cant get head heart ...   \n",
            "\n",
            "                                     normalized_text  \\\n",
            "0  my wife suggested i read this book and i resis...   \n",
            "1  great story of the us ambassador to germany an...   \n",
            "2  i cracked and finally picked this up very enjo...   \n",
            "3  loved every minute so sad there isnt another i...   \n",
            "4  i finished reading this days ago and cant get ...   \n",
            "\n",
            "                                     lemmatized_text  \\\n",
            "0  my wife suggest I read this book and I resist ...   \n",
            "1  great story of the us ambassador to germany an...   \n",
            "2  I crack and finally pick this up very enjoyabl...   \n",
            "3  love every minute so sad there be not another ...   \n",
            "4  I finish read this day ago and can not get thi...   \n",
            "\n",
            "                                     pos_tagged_text  \\\n",
            "0  [(my, PRON), (wife, NOUN), (suggest, VERB), (I...   \n",
            "1  [(great, ADJ), (story, NOUN), (of, ADP), (the,...   \n",
            "2  [(I, PRON), (crack, VERB), (and, CCONJ), (fina...   \n",
            "3  [(love, NOUN), (every, DET), (minute, NOUN), (...   \n",
            "4  [(I, PRON), (finish, VERB), (read, VERB), (thi...   \n",
            "\n",
            "                                      named_entities  \n",
            "0          [(la, GPE), (seattle, GPE), (india, GPE)]  \n",
            "1  [(us, GPE), (germany, GPE), (martha, PERSON), ...  \n",
            "2                                [(each year, DATE)]  \n",
            "3  [(every minute, TIME), (a month later, DATE), ...  \n",
            "4                             [(this day ago, DATE)]  \n"
          ]
        }
      ],
      "source": [
        "print(df[['review_text', 'cleaned_review_text', 'normalized_text_no_stopwords', 'normalized_text', 'lemmatized_text', 'pos_tagged_text', 'named_entities']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32dab1c3",
        "outputId": "8cfc662b-1084-4fcd-a8b0-b36efb16242a"
      },
      "outputs": [],
      "source": [
        "# # Sort the DataFrame by the length of the 'review_text' column\n",
        "# df_sorted_by_review_length = df.copy()\n",
        "# df_sorted_by_review_length['review_text_length'] = df_sorted_by_review_length['review_text'].apply(len)\n",
        "# df_sorted_by_review_length = df_sorted_by_review_length.sort_values(by='review_text_length', ascending=True)\n",
        "# print(df_sorted_by_review_length[['review_text', 'cleaned_review_text', 'normalized_text_no_stopwords', 'normalized_text', 'lemmatized_text', 'pos_tagged_text', 'named_entities']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"./data/book_processed_output.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947fa7cd",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "find number of suitable book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540e111",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd75454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1521962\n",
      "Columns: ['books_count', 'reviews_count', 'original_publication_month', 'default_description_language_code', 'text_reviews_count', 'best_book_id', 'original_publication_year', 'original_title', 'rating_dist', 'default_chaptering_book_id', 'original_publication_day', 'original_language_id', 'ratings_count', 'media_type', 'ratings_sum', 'work_id']\n",
      "\n",
      "First few rows:\n",
      "  books_count reviews_count original_publication_month  \\\n",
      "0           1             6                          8   \n",
      "1          22         10162                              \n",
      "2           2           268                              \n",
      "3          38         89252                          7   \n",
      "4           2            49                              \n",
      "\n",
      "  default_description_language_code text_reviews_count best_book_id  \\\n",
      "0                                                    1      5333265   \n",
      "1                                                  741        25717   \n",
      "2                                                    7      7327624   \n",
      "3                                                 3504      6066819   \n",
      "4                                                    5       287140   \n",
      "\n",
      "  original_publication_year  \\\n",
      "0                      1984   \n",
      "1                      2001   \n",
      "2                      1987   \n",
      "3                      2009   \n",
      "4                      1990   \n",
      "\n",
      "                                      original_title  \\\n",
      "0                       W. C. Fields: A Life on Film   \n",
      "1                                        Good Harbor   \n",
      "2                                                      \n",
      "3                               Best Friends Forever   \n",
      "4  Runic Astrology: Starcraft and Timekeeping in ...   \n",
      "\n",
      "                                        rating_dist  \\\n",
      "0                       5:1|4:1|3:1|2:0|1:0|total:3   \n",
      "1        5:517|4:1787|3:2763|2:966|1:196|total:6229   \n",
      "2                  5:49|4:58|3:26|2:5|1:3|total:141   \n",
      "3  5:9152|4:16855|3:19507|2:6210|1:1549|total:53273   \n",
      "4                      5:6|4:1|3:3|2:3|1:2|total:15   \n",
      "\n",
      "  default_chaptering_book_id original_publication_day original_language_id  \\\n",
      "0                                                                            \n",
      "1                                                                            \n",
      "2                                                                            \n",
      "3                                                  14                        \n",
      "4                                                                            \n",
      "\n",
      "  ratings_count media_type ratings_sum  work_id  \n",
      "0             3       book          12  5400751  \n",
      "1          6229       book       20150  1323437  \n",
      "2           141       book         568  8948723  \n",
      "3         53273       book      185670  6243154  \n",
      "4            15       book          51   278577  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the goodreads_book_works.json file (JSONL format - one JSON object per line)\n",
    "data = []\n",
    "with open('./data/goodreads_book_works.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the shape and columns\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbb999",
   "metadata": {},
   "source": [
    "## data inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb6ff93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking structure of goodreads_reviews_dedup.json...\n",
      "\n",
      "Sample review 1:\n",
      "Keys: dict_keys(['user_id', 'book_id', 'review_id', 'rating', 'review_text', 'date_added', 'date_updated', 'read_at', 'started_at', 'n_votes', 'n_comments'])\n",
      "Content: {'user_id': '8842281e1d1347389f2ab93d60773d4d', 'book_id': '24375664', 'review_id': '5cd416f3efc3f944fce4ce2db2290d5e', 'rating': 5, 'review_text': \"Mind blowingly cool. Best science fiction I've read in some time. I just loved all the descriptions of the society of the future - how they lived in trees, the notion of owning property or even getting married was gone. How every surface was a screen. \\n The undulations of how society responds to the Trisolaran threat seem surprising to me. Maybe its more the Chinese perspective, but I wouldn't have thought the ETO would exist in book 1, and I wouldn't have thought people would get so over-confident in our primitive fleet's chances given you have to think that with superior science they would have weapons - and defenses - that would just be as rifles to arrows once were. \\n But the moment when Luo Ji won as a wallfacer was just too cool. I may have actually done a fist pump. Though by the way, if the Dark Forest theory is right - and I see no reason why it wouldn't be - we as a society should probably stop broadcasting so much signal out into the universe.\", 'date_added': 'Fri Aug 25 13:55:02 -0700 2017', 'date_updated': 'Mon Oct 09 08:55:59 -0700 2017', 'read_at': 'Sat Oct 07 00:00:00 -0700 2017', 'started_at': 'Sat Aug 26 00:00:00 -0700 2017', 'n_votes': 16, 'n_comments': 0}\n"
     ]
    }
   ],
   "source": [
    "# First, let's check the structure of the reviews file\n",
    "print(\"Checking structure of goodreads_reviews_dedup.json...\")\n",
    "with open('./data/goodreads_reviews_dedup.json', 'r', encoding='utf-8') as f:\n",
    "    for i in range(3):\n",
    "        line = f.readline()\n",
    "        if line.strip():\n",
    "            review = json.loads(line)\n",
    "            print(f\"\\nSample review {i+1}:\")\n",
    "            print(f\"Keys: {review.keys()}\")\n",
    "            print(f\"Content: {review}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00a452",
   "metadata": {},
   "source": [
    "## review count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad6f9e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting actual reviews per book from goodreads_reviews_dedup.json...\n",
      "Processed 100000 lines...\n",
      "Processed 200000 lines...\n",
      "Processed 300000 lines...\n",
      "Processed 400000 lines...\n",
      "Processed 500000 lines...\n",
      "Processed 600000 lines...\n",
      "Processed 700000 lines...\n",
      "Processed 800000 lines...\n",
      "Processed 900000 lines...\n",
      "Processed 1000000 lines...\n",
      "Processed 1100000 lines...\n",
      "Processed 1200000 lines...\n",
      "Processed 1300000 lines...\n",
      "Processed 1400000 lines...\n",
      "Processed 1500000 lines...\n",
      "Processed 1600000 lines...\n",
      "Processed 1700000 lines...\n",
      "Processed 1800000 lines...\n",
      "Processed 1900000 lines...\n",
      "Processed 2000000 lines...\n",
      "Processed 2100000 lines...\n",
      "Processed 2200000 lines...\n",
      "Processed 2300000 lines...\n",
      "Processed 2400000 lines...\n",
      "Processed 2500000 lines...\n",
      "Processed 2600000 lines...\n",
      "Processed 2700000 lines...\n",
      "Processed 2800000 lines...\n",
      "Processed 2900000 lines...\n",
      "Processed 3000000 lines...\n",
      "Processed 3100000 lines...\n",
      "Processed 3200000 lines...\n",
      "Processed 3300000 lines...\n",
      "Processed 3400000 lines...\n",
      "Processed 3500000 lines...\n",
      "Processed 3600000 lines...\n",
      "Processed 3700000 lines...\n",
      "Processed 3800000 lines...\n",
      "Processed 3900000 lines...\n",
      "Processed 4000000 lines...\n",
      "Processed 4100000 lines...\n",
      "Processed 4200000 lines...\n",
      "Processed 4300000 lines...\n",
      "Processed 4400000 lines...\n",
      "Processed 4500000 lines...\n",
      "Processed 4600000 lines...\n",
      "Processed 4700000 lines...\n",
      "Processed 4800000 lines...\n",
      "Processed 4900000 lines...\n",
      "Processed 5000000 lines...\n",
      "Processed 5100000 lines...\n",
      "Processed 5200000 lines...\n",
      "Processed 5300000 lines...\n",
      "Processed 5400000 lines...\n",
      "Processed 5500000 lines...\n",
      "Processed 5600000 lines...\n",
      "Processed 5700000 lines...\n",
      "Processed 5800000 lines...\n",
      "Processed 5900000 lines...\n",
      "Processed 6000000 lines...\n",
      "Processed 6100000 lines...\n",
      "Processed 6200000 lines...\n",
      "Processed 6300000 lines...\n",
      "Processed 6400000 lines...\n",
      "Processed 6500000 lines...\n",
      "Processed 6600000 lines...\n",
      "Processed 6700000 lines...\n",
      "Processed 6800000 lines...\n",
      "Processed 6900000 lines...\n",
      "Processed 7000000 lines...\n",
      "Processed 7100000 lines...\n",
      "Processed 7200000 lines...\n",
      "Processed 7300000 lines...\n",
      "Processed 7400000 lines...\n",
      "Processed 7500000 lines...\n",
      "Processed 7600000 lines...\n",
      "Processed 7700000 lines...\n",
      "Processed 7800000 lines...\n",
      "Processed 7900000 lines...\n",
      "Processed 8000000 lines...\n",
      "Processed 8100000 lines...\n",
      "Processed 8200000 lines...\n",
      "Processed 8300000 lines...\n",
      "Processed 8400000 lines...\n",
      "Processed 8500000 lines...\n",
      "Processed 8600000 lines...\n",
      "Processed 8700000 lines...\n",
      "Processed 8800000 lines...\n",
      "Processed 8900000 lines...\n",
      "Processed 9000000 lines...\n",
      "Processed 9100000 lines...\n",
      "Processed 9200000 lines...\n",
      "Processed 9300000 lines...\n",
      "Processed 9400000 lines...\n",
      "Processed 9500000 lines...\n",
      "Processed 9600000 lines...\n",
      "Processed 9700000 lines...\n",
      "Processed 9800000 lines...\n",
      "Processed 9900000 lines...\n",
      "Processed 10000000 lines...\n",
      "Processed 10100000 lines...\n",
      "Processed 10200000 lines...\n",
      "Processed 10300000 lines...\n",
      "Processed 10400000 lines...\n",
      "Processed 10500000 lines...\n",
      "Processed 10600000 lines...\n",
      "Processed 10700000 lines...\n",
      "Processed 10800000 lines...\n",
      "Processed 10900000 lines...\n",
      "Processed 11000000 lines...\n",
      "Processed 11100000 lines...\n",
      "Processed 11200000 lines...\n",
      "Processed 11300000 lines...\n",
      "Processed 11400000 lines...\n",
      "Processed 11500000 lines...\n",
      "Processed 11600000 lines...\n",
      "Processed 11700000 lines...\n",
      "Processed 11800000 lines...\n",
      "Processed 11900000 lines...\n",
      "Processed 12000000 lines...\n",
      "Processed 12100000 lines...\n",
      "Processed 12200000 lines...\n",
      "Processed 12300000 lines...\n",
      "Processed 12400000 lines...\n",
      "Processed 12500000 lines...\n",
      "Processed 12600000 lines...\n",
      "Processed 12700000 lines...\n",
      "Processed 12800000 lines...\n",
      "Processed 12900000 lines...\n",
      "Processed 13000000 lines...\n",
      "Processed 13100000 lines...\n",
      "Processed 13200000 lines...\n",
      "Processed 13300000 lines...\n",
      "Processed 13400000 lines...\n",
      "Processed 13500000 lines...\n",
      "Processed 13600000 lines...\n",
      "Processed 13700000 lines...\n",
      "Processed 13800000 lines...\n",
      "Processed 13900000 lines...\n",
      "Processed 14000000 lines...\n",
      "Processed 14100000 lines...\n",
      "Processed 14200000 lines...\n",
      "Processed 14300000 lines...\n",
      "Processed 14400000 lines...\n",
      "Processed 14500000 lines...\n",
      "Processed 14600000 lines...\n",
      "Processed 14700000 lines...\n",
      "Processed 14800000 lines...\n",
      "Processed 14900000 lines...\n",
      "Processed 15000000 lines...\n",
      "Processed 15100000 lines...\n",
      "Processed 15200000 lines...\n",
      "Processed 15300000 lines...\n",
      "Processed 15400000 lines...\n",
      "Processed 15500000 lines...\n",
      "Processed 15600000 lines...\n",
      "Processed 15700000 lines...\n",
      "Total lines processed: 15739967\n",
      "\n",
      "Books with actual review count > 1250: 726\n",
      "\n",
      "====================================================================================================\n",
      "Top 20 books by actual review count (> 1250):\n",
      "\n",
      " 1. Book ID 11870085: 20756 reviews\n",
      " 2. Book ID 2767052: 18617 reviews\n",
      " 3. Book ID 7260188: 13536 reviews\n",
      " 4. Book ID 22557272: 13402 reviews\n",
      " 5. Book ID 6148028: 11904 reviews\n",
      " 6. Book ID 19063: 11300 reviews\n",
      " 7. Book ID 10818853: 11184 reviews\n",
      " 8. Book ID 13335037: 10743 reviews\n",
      " 9. Book ID 41865: 10535 reviews\n",
      "10. Book ID 15745753: 9590 reviews\n",
      "11. Book ID 18007564: 9590 reviews\n",
      "12. Book ID 11235712: 9585 reviews\n",
      "13. Book ID 15507958: 9557 reviews\n",
      "14. Book ID 9460487: 9557 reviews\n",
      "15. Book ID 8442457: 9316 reviews\n",
      "16. Book ID 11735983: 9207 reviews\n",
      "17. Book ID 29056083: 9108 reviews\n",
      "18. Book ID 256683: 8841 reviews\n",
      "19. Book ID 9969571: 8672 reviews\n",
      "20. Book ID 9361589: 8668 reviews\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Total books with > 1250 reviews: 726\n",
      "Review count range: 1252 to 20756\n"
     ]
    }
   ],
   "source": [
    "# Count actual reviews per book from the reviews file\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Counting actual reviews per book from goodreads_reviews_dedup.json...\")\n",
    "\n",
    "review_counts = Counter()\n",
    "lines_processed = 0\n",
    "\n",
    "with open('./data/goodreads_reviews_dedup.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lines_processed += 1\n",
    "        if lines_processed % 100000 == 0:\n",
    "            print(f\"Processed {lines_processed} lines...\")\n",
    "        \n",
    "        if line.strip():\n",
    "            try:\n",
    "                review = json.loads(line)\n",
    "                book_id = int(review.get('book_id', -1))\n",
    "                review_counts[book_id] += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(f\"Total lines processed: {lines_processed}\\n\")\n",
    "\n",
    "# Find books with actual review count > 1250\n",
    "target = 1250\n",
    "books_above_target = {book_id: count for book_id, count in review_counts.items() \n",
    "                      if count > target}\n",
    "\n",
    "print(f\"Books with actual review count > {target}: {len(books_above_target)}\\n\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"Top 20 books by actual review count (> {target}):\\n\")\n",
    "\n",
    "# Sort and display top books\n",
    "sorted_books = sorted(books_above_target.items(), key=lambda x: x[1], reverse=True)\n",
    "for idx, (book_id, count) in enumerate(sorted_books[:20], 1):\n",
    "    print(f\"{idx:2d}. Book ID {book_id}: {count} reviews\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"\\nTotal books with > {target} reviews: {len(books_above_target)}\")\n",
    "print(f\"Review count range: {min(books_above_target.values())} to {max(books_above_target.values())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54490a",
   "metadata": {},
   "source": [
    "## genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c4b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading genre data from goodreads_book_genres_initial.json...\n",
      "Loaded genres for 2360655 books\n",
      "\n",
      "Books from books_above_target with genre data: 726\n",
      "\n",
      "Genre distribution (top 20):\n",
      "  fiction: 260 books\n",
      "  young-adult: 159 books\n",
      "  fantasy, paranormal: 157 books\n",
      "  romance: 59 books\n",
      "  non-fiction: 50 books\n",
      "  mystery, thriller, crime: 26 books\n",
      "  comics, graphic: 7 books\n",
      "  children: 4 books\n",
      "  history, historical fiction, biography: 3 books\n",
      "  poetry: 1 books\n",
      "\n",
      "====================================================================================================\n",
      "Selected 8 books with different genres:\n",
      "\n",
      "1. Book ID 23513349: 2814 reviews | Primary Genre: poetry\n",
      "2. Book ID 9938498: 1518 reviews | Primary Genre: history, historical fiction, biography\n",
      "3. Book ID 3636: 6156 reviews | Primary Genre: children\n",
      "4. Book ID 18659623: 1493 reviews | Primary Genre: comics, graphic\n",
      "5. Book ID 6218281: 1952 reviews | Primary Genre: mystery, thriller, crime\n",
      "6. Book ID 27161156: 1911 reviews | Primary Genre: non-fiction\n",
      "7. Book ID 15507958: 9557 reviews | Primary Genre: romance\n",
      "8. Book ID 136251: 6486 reviews | Primary Genre: fantasy, paranormal\n",
      "9. Book ID 2767052: 18617 reviews | Primary Genre: young-adult\n",
      "10. Book ID 13526165: 3280 reviews | Primary Genre: fiction\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "books_genre_1250_above_ids = [23513349, 9938498, 3636, 18659623, 6218281, 27161156, 15507958, 136251, 2767052, 13526165]\n"
     ]
    }
   ],
   "source": [
    "# Load genre data and find 8 books with different genres from books_above_target\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Loading genre data from goodreads_book_genres_initial.json...\")\n",
    "\n",
    "# Build a mapping of book_id -> genres\n",
    "book_genres_map = {}\n",
    "\n",
    "with open('./data/goodreads_book_genres_initial.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                book_id = int(entry.get('book_id', -1))\n",
    "                genres = entry.get('genres', {})\n",
    "                book_genres_map[book_id] = genres\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(f\"Loaded genres for {len(book_genres_map)} books\\n\")\n",
    "\n",
    "# Find books from books_above_target that have genre data\n",
    "books_with_genres = {}\n",
    "for book_id in books_above_target.keys():\n",
    "    if book_id in book_genres_map:\n",
    "        books_with_genres[book_id] = book_genres_map[book_id]\n",
    "\n",
    "print(f\"Books from books_above_target with genre data: {len(books_with_genres)}\\n\")\n",
    "\n",
    "# Select 8 books with different genres\n",
    "# Strategy: group by primary genre and pick one from each group\n",
    "genre_to_books = defaultdict(list)\n",
    "\n",
    "for book_id, genres in books_with_genres.items():\n",
    "    if isinstance(genres, dict) and genres:  # genres is a dict\n",
    "        # Get the first (primary) genre from the dict keys\n",
    "        primary_genre = list(genres.keys())[0]\n",
    "        genre_to_books[primary_genre].append(book_id)\n",
    "    elif isinstance(genres, list) and genres:  # In case genres is a list\n",
    "        primary_genre = genres[0]\n",
    "        genre_to_books[primary_genre].append(book_id)\n",
    "\n",
    "print(\"Genre distribution (top 20):\")\n",
    "sorted_genres_list = sorted(genre_to_books.keys(), key=lambda g: len(genre_to_books[g]), reverse=True)\n",
    "for genre in sorted_genres_list[:20]:\n",
    "    print(f\"  {genre}: {len(genre_to_books[genre])} books\")\n",
    "\n",
    "# Select 8 books with different genres (one per genre, prioritizing genres with fewer books)\n",
    "books_genre_1250_above_ids = []\n",
    "selected_genres = []\n",
    "\n",
    "# Sort genres by number of books (pick from genres with fewer books first for diversity)\n",
    "sorted_genres_by_count = sorted(genre_to_books.keys(), key=lambda g: len(genre_to_books[g]))\n",
    "\n",
    "for genre in sorted_genres_by_count:\n",
    "    if len(books_genre_1250_above_ids) < len(sorted_genres_list):\n",
    "        book_id = genre_to_books[genre][0]\n",
    "        books_genre_1250_above_ids.append(book_id)\n",
    "        selected_genres.append(genre)\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(f\"Selected 10 books with different genres:\\n\")\n",
    "\n",
    "for idx, (book_id, genre) in enumerate(zip(books_genre_1250_above_ids, selected_genres), 1):\n",
    "    review_count = books_above_target[book_id]\n",
    "    print(f\"{idx}. Book ID {book_id}: {review_count} reviews | Primary Genre: {genre}\")\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(f\"\\nbooks_genre_1250_above_ids = {books_genre_1250_above_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e2c8b",
   "metadata": {},
   "source": [
    "## extract review\n",
    "from chosen book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d01d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting reviews for 10 books with different genres (actual review count > 1250)...\n",
      "\n",
      "Processed 100,000 lines, found 250 matches...\n",
      "Processed 200,000 lines, found 500 matches...\n",
      "Processed 300,000 lines, found 755 matches...\n",
      "Processed 400,000 lines, found 1,007 matches...\n",
      "Processed 500,000 lines, found 1,263 matches...\n",
      "Processed 600,000 lines, found 1,535 matches...\n",
      "Processed 700,000 lines, found 1,791 matches...\n",
      "Processed 800,000 lines, found 2,044 matches...\n",
      "Processed 900,000 lines, found 2,296 matches...\n",
      "Processed 1,000,000 lines, found 2,567 matches...\n",
      "Processed 1,100,000 lines, found 2,821 matches...\n",
      "Processed 1,200,000 lines, found 3,097 matches...\n",
      "Processed 1,300,000 lines, found 3,342 matches...\n",
      "Processed 1,400,000 lines, found 3,633 matches...\n",
      "Processed 1,500,000 lines, found 3,905 matches...\n",
      "Processed 1,600,000 lines, found 4,165 matches...\n",
      "Processed 1,700,000 lines, found 4,422 matches...\n",
      "Processed 1,800,000 lines, found 4,662 matches...\n",
      "Processed 1,900,000 lines, found 4,934 matches...\n",
      "Processed 2,000,000 lines, found 5,174 matches...\n",
      "Processed 2,100,000 lines, found 5,430 matches...\n",
      "Processed 2,200,000 lines, found 5,695 matches...\n",
      "Processed 2,300,000 lines, found 5,965 matches...\n",
      "Processed 2,400,000 lines, found 6,244 matches...\n",
      "Processed 2,500,000 lines, found 6,514 matches...\n",
      "Processed 2,600,000 lines, found 6,763 matches...\n",
      "Processed 2,700,000 lines, found 6,992 matches...\n",
      "Processed 2,800,000 lines, found 7,209 matches...\n",
      "Processed 2,900,000 lines, found 7,463 matches...\n",
      "Processed 3,000,000 lines, found 7,753 matches...\n",
      "Processed 3,100,000 lines, found 8,004 matches...\n",
      "Processed 3,200,000 lines, found 8,278 matches...\n",
      "Processed 3,300,000 lines, found 8,552 matches...\n",
      "Processed 3,400,000 lines, found 8,849 matches...\n",
      "Processed 3,500,000 lines, found 9,109 matches...\n",
      "Processed 3,600,000 lines, found 9,383 matches...\n",
      "Processed 3,700,000 lines, found 9,614 matches...\n",
      "Processed 3,800,000 lines, found 9,897 matches...\n",
      "Processed 3,900,000 lines, found 10,127 matches...\n",
      "Processed 4,000,000 lines, found 10,381 matches...\n",
      "Processed 4,100,000 lines, found 10,637 matches...\n",
      "Processed 4,200,000 lines, found 10,905 matches...\n",
      "Processed 4,300,000 lines, found 11,192 matches...\n",
      "Processed 4,400,000 lines, found 11,456 matches...\n",
      "Processed 4,500,000 lines, found 11,749 matches...\n",
      "Processed 4,600,000 lines, found 12,025 matches...\n",
      "Processed 4,700,000 lines, found 12,298 matches...\n",
      "Processed 4,800,000 lines, found 12,563 matches...\n",
      "Processed 4,900,000 lines, found 12,805 matches...\n",
      "Processed 5,000,000 lines, found 13,058 matches...\n",
      "Processed 5,100,000 lines, found 13,331 matches...\n",
      "Processed 5,200,000 lines, found 13,587 matches...\n",
      "Processed 5,300,000 lines, found 13,871 matches...\n",
      "Processed 5,400,000 lines, found 14,140 matches...\n",
      "Processed 5,500,000 lines, found 14,400 matches...\n",
      "Processed 5,600,000 lines, found 14,652 matches...\n",
      "Processed 5,700,000 lines, found 14,945 matches...\n",
      "Processed 5,800,000 lines, found 15,242 matches...\n",
      "Processed 5,900,000 lines, found 15,510 matches...\n",
      "Processed 6,000,000 lines, found 15,782 matches...\n",
      "Processed 6,100,000 lines, found 16,035 matches...\n",
      "Processed 6,200,000 lines, found 16,269 matches...\n",
      "Processed 6,300,000 lines, found 16,544 matches...\n",
      "Processed 6,400,000 lines, found 16,830 matches...\n",
      "Processed 6,500,000 lines, found 17,109 matches...\n",
      "Processed 6,600,000 lines, found 17,378 matches...\n",
      "Processed 6,700,000 lines, found 17,683 matches...\n",
      "Processed 6,800,000 lines, found 17,966 matches...\n",
      "Processed 6,900,000 lines, found 18,240 matches...\n",
      "Processed 7,000,000 lines, found 18,476 matches...\n",
      "Processed 7,100,000 lines, found 18,780 matches...\n",
      "Processed 7,200,000 lines, found 19,054 matches...\n",
      "Processed 7,300,000 lines, found 19,335 matches...\n",
      "Processed 7,400,000 lines, found 19,601 matches...\n",
      "Processed 7,500,000 lines, found 19,934 matches...\n",
      "Processed 7,600,000 lines, found 20,179 matches...\n",
      "Processed 7,700,000 lines, found 20,473 matches...\n",
      "Processed 7,800,000 lines, found 20,799 matches...\n",
      "Processed 7,900,000 lines, found 21,046 matches...\n",
      "Processed 8,000,000 lines, found 21,302 matches...\n",
      "Processed 8,100,000 lines, found 21,576 matches...\n",
      "Processed 8,200,000 lines, found 21,875 matches...\n",
      "Processed 8,300,000 lines, found 22,147 matches...\n",
      "Processed 8,400,000 lines, found 22,436 matches...\n",
      "Processed 8,500,000 lines, found 22,723 matches...\n",
      "Processed 8,600,000 lines, found 23,049 matches...\n",
      "Processed 8,700,000 lines, found 23,341 matches...\n",
      "Processed 8,800,000 lines, found 23,666 matches...\n",
      "Processed 8,900,000 lines, found 23,939 matches...\n",
      "Processed 9,000,000 lines, found 24,219 matches...\n",
      "Processed 9,100,000 lines, found 24,472 matches...\n",
      "Processed 9,200,000 lines, found 24,747 matches...\n",
      "Processed 9,300,000 lines, found 25,018 matches...\n",
      "Processed 9,400,000 lines, found 25,324 matches...\n",
      "Processed 9,500,000 lines, found 25,616 matches...\n",
      "Processed 9,600,000 lines, found 25,946 matches...\n",
      "Processed 9,700,000 lines, found 26,200 matches...\n",
      "Processed 9,800,000 lines, found 26,431 matches...\n",
      "Processed 9,900,000 lines, found 26,745 matches...\n",
      "Processed 10,000,000 lines, found 26,973 matches...\n",
      "Processed 10,100,000 lines, found 27,275 matches...\n",
      "Processed 10,200,000 lines, found 27,588 matches...\n",
      "Processed 10,300,000 lines, found 27,899 matches...\n",
      "Processed 10,400,000 lines, found 28,189 matches...\n",
      "Processed 10,500,000 lines, found 28,495 matches...\n",
      "Processed 10,600,000 lines, found 28,770 matches...\n",
      "Processed 10,700,000 lines, found 29,093 matches...\n",
      "Processed 10,800,000 lines, found 29,383 matches...\n",
      "Processed 10,900,000 lines, found 29,716 matches...\n",
      "Processed 11,000,000 lines, found 30,024 matches...\n",
      "Processed 11,100,000 lines, found 30,328 matches...\n",
      "Processed 11,200,000 lines, found 30,630 matches...\n",
      "Processed 11,300,000 lines, found 30,920 matches...\n",
      "Processed 11,400,000 lines, found 31,216 matches...\n",
      "Processed 11,500,000 lines, found 31,526 matches...\n",
      "Processed 11,600,000 lines, found 31,834 matches...\n",
      "Processed 11,700,000 lines, found 32,123 matches...\n",
      "Processed 11,800,000 lines, found 32,436 matches...\n",
      "Processed 11,900,000 lines, found 32,689 matches...\n",
      "Processed 12,000,000 lines, found 32,958 matches...\n",
      "Processed 12,100,000 lines, found 33,252 matches...\n",
      "Processed 12,200,000 lines, found 33,539 matches...\n",
      "Processed 12,300,000 lines, found 33,836 matches...\n",
      "Processed 12,400,000 lines, found 34,119 matches...\n",
      "Processed 12,500,000 lines, found 34,426 matches...\n",
      "Processed 12,600,000 lines, found 34,755 matches...\n",
      "Processed 12,700,000 lines, found 35,029 matches...\n",
      "Processed 12,800,000 lines, found 35,275 matches...\n",
      "Processed 12,900,000 lines, found 35,548 matches...\n",
      "Processed 13,000,000 lines, found 35,802 matches...\n",
      "Processed 13,100,000 lines, found 36,067 matches...\n",
      "Processed 13,200,000 lines, found 36,357 matches...\n",
      "Processed 13,300,000 lines, found 36,632 matches...\n",
      "Processed 13,400,000 lines, found 36,922 matches...\n",
      "Processed 13,500,000 lines, found 37,213 matches...\n",
      "Processed 13,600,000 lines, found 37,517 matches...\n",
      "Processed 13,700,000 lines, found 37,750 matches...\n",
      "Processed 13,800,000 lines, found 38,048 matches...\n",
      "Processed 13,900,000 lines, found 38,306 matches...\n",
      "Processed 14,000,000 lines, found 38,587 matches...\n",
      "Processed 14,100,000 lines, found 38,849 matches...\n",
      "Processed 14,200,000 lines, found 39,103 matches...\n",
      "Processed 14,300,000 lines, found 39,348 matches...\n",
      "Processed 14,400,000 lines, found 39,639 matches...\n",
      "Processed 14,500,000 lines, found 39,922 matches...\n",
      "Processed 14,600,000 lines, found 40,184 matches...\n",
      "Processed 14,700,000 lines, found 40,435 matches...\n",
      "Processed 14,800,000 lines, found 40,725 matches...\n",
      "Processed 14,900,000 lines, found 41,020 matches...\n",
      "Processed 15,000,000 lines, found 41,593 matches...\n",
      "Processed 15,100,000 lines, found 42,167 matches...\n",
      "Processed 15,200,000 lines, found 42,788 matches...\n",
      "Processed 15,300,000 lines, found 43,434 matches...\n",
      "Processed 15,400,000 lines, found 44,042 matches...\n",
      "Processed 15,500,000 lines, found 44,716 matches...\n",
      "Processed 15,600,000 lines, found 45,424 matches...\n",
      "Processed 15,700,000 lines, found 46,082 matches...\n",
      "\n",
      "Total reviews found: 46,358\n",
      "\n",
      "Successfully saved 46,358 reviews to './data/books_genre_1250_above_reviews.json'\n",
      "\n",
      "========================================================================================================================\n",
      "Summary of reviews by book:\n",
      "\n",
      "  Book ID 3636: 5365 reviews\n",
      "  Book ID 136251: 5496 reviews\n",
      "  Book ID 2767052: 15937 reviews\n",
      "  Book ID 6218281: 1764 reviews\n",
      "  Book ID 9938498: 1370 reviews\n",
      "  Book ID 13526165: 2924 reviews\n",
      "  Book ID 15507958: 7902 reviews\n",
      "  Book ID 18659623: 1312 reviews\n",
      "  Book ID 23513349: 2561 reviews\n",
      "  Book ID 27161156: 1727 reviews\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Extract reviews for the 8 books with different genres (actual review count > 1250)\n",
    "print(f\"Extracting reviews for {len(books_genre_1250_above_ids)} books with different genres (actual review count > 1250)...\\n\")\n",
    "\n",
    "reviews_1250_above = []\n",
    "review_count = 0\n",
    "lines_processed = 0\n",
    "\n",
    "input_file = './data/goodreads_reviews_dedup.json'\n",
    "output_file = './data/books_genre_1250_above_reviews.json'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lines_processed += 1\n",
    "\n",
    "        # Progress print\n",
    "        if lines_processed % 100000 == 0:\n",
    "            print(f\"Processed {lines_processed:,} lines, found {review_count:,} matches...\")\n",
    "\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            review = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        review_book_id = int(review.get('book_id', -1))\n",
    "\n",
    "        if review_book_id in books_genre_1250_above_ids:\n",
    "            # Filter out reviews containing \"...\" (truncated reviews)\n",
    "            review_text = review.get('review_text', '')\n",
    "            if review_text and '...' not in review_text:\n",
    "                reviews_1250_above.append(review)\n",
    "                review_count += 1\n",
    "\n",
    "print(f\"\\nTotal reviews found: {review_count:,}\\n\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# SAVE AS VALID JSON ARRAY\n",
    "# ------------------------------------------\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('[\\n')\n",
    "    for idx, review in enumerate(reviews_1250_above):\n",
    "        f.write(json.dumps(review, ensure_ascii=False))\n",
    "        if idx < len(reviews_1250_above) - 1:\n",
    "            f.write(',\\n')\n",
    "    f.write('\\n]')\n",
    "\n",
    "print(f\"Successfully saved {review_count:,} reviews to '{output_file}'\\n\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# SUMMARY BY BOOK\n",
    "# ------------------------------------------\n",
    "\n",
    "if review_count > 0:\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Summary of reviews by book:\\n\")\n",
    "\n",
    "    book_review_counts = Counter()\n",
    "\n",
    "    for review in reviews_1250_above:\n",
    "        book_id = int(review.get('book_id', -1))\n",
    "        book_review_counts[book_id] += 1\n",
    "\n",
    "    # Display sorted by book_id\n",
    "    for book_id in sorted(book_review_counts.keys()):\n",
    "        count = book_review_counts[book_id]\n",
    "        print(f\"  Book ID {book_id}: {count} reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aedd00",
   "metadata": {},
   "source": [
    "## JSON to CSV Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f36b4c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting books_1250_above_reviews.json to CSV format...\n",
      "\n",
      "Successfully converted to CSV!\n",
      "File: ./data/books_1250_above_reviews.csv\n",
      "Total rows: 46,358\n",
      "Columns: book_id, user_id, review_id, rating, review_text, date_added, date_updated, n_votes, n_comments\n",
      "\n",
      "========================================================================================================================\n",
      "Sample of CSV data (first 5 rows):\n",
      "\n",
      " book_id                          user_id                        review_id  rating                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            review_text                     date_added                   date_updated  n_votes  n_comments\n",
      "13526165 8842281e1d1347389f2ab93d60773d4d 51fe3e46c7f8eb39f5623d1bd8bbbbfc       5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           My wife suggested I read this book, and I resisted for a while as my impression of it somehow was \"not for me\" - but I was wrong! I'm glad she finally convinced me, because it was fun and I couldn't put it down after I got ~20% in. \\n Bernadette moved to Seattle from LA, abandons her career as an architect to have kids, hates Seattle, and basically loses it over the course of the book. The book was fun and written a light, humorous tone - from making fun of Seattle and Microsoft, the obsession of parents and how serious they take themselves (I loved how she called the other parents \"gnats\"), to how she hires a virtual assistant in India. \\n But the most fun part of the book is just how wacky Bernadette is. You really do have to make life fun - it won't do it for you. \\n \"That's right,' she told the girls. 'You are bored. And I'm going to let you in on a little secret about life. You think it's boring now? Well, it only gets more boring. The sooner you learn it's on you to make life interesting, the better off you'll be.\" Thu May 30 20:25:19 -0700 2013 Wed Mar 22 11:47:21 -0700 2017       23           5\n",
      " 9938498 8842281e1d1347389f2ab93d60773d4d bff5654c639c7b008571c3d4398d930a       4 Great story of the US Ambassador to Germany and his experience during the build-up of ww2 from 1933 - 1937. Nothing new or exciting really revealed, and I'm sure there are many other accounts of the buildup to ww2 out there, but this one told it from the perspective of mostly the Ambassador, Dodd, and his daughter, Martha. The story was engaging and well-written, and thus interesting. I think the ending fell pretty flat - I was hoping for more. \\n Reading about the Nazis one is of course always appalled at how they got to be in power and commit such atrocities, and this book did provide some clues. Hitler seemed primarily willing to tell people what they needed and wanted to hear, even if it wasn't the truth - and he was a great storyteller. He also used fear as a primary weapon, as there were many in Germany not behind him - but they were too afraid to make a stand or speak up. \\n I suppose the climax of the book was the Night of the Long Knives, where Hitler arrested and executed all of his major opponents, clearing the way for his absolute grip on the nation. He ruthlessly murdered an estimated 77 to several hundred people (the count was never really known) and played it off as putting down a rebellion, which much of the world bought for many years. \\n I think one of the more interesting pieces of the book was watching Dodd (and Martha) go from thinking the Nazis were good for Germany, to realizing how terrible they were and hoping for them to be overthrown. It was a slow gradual process, and one that I'm sure was not immediately obvious to anyone. In the end, I think the book is aptly named. Sat May 21 15:04:50 -0700 2011 Wed Mar 22 11:46:56 -0700 2017        5           1\n",
      " 2767052 8842281e1d1347389f2ab93d60773d4d 248c011811e945eca861b5c31a549291       5                                                                                                                                                                                                                                                                                                      I cracked and finally picked this up. Very enjoyable quick read - couldn't put it down - it was like crack. \\n I'm a bit bothered by the lack of backstory of how Panem and the Hunger Games come about. It is just kind of explained away in a few paragraphs and we are left to accept this very strange world where teenagers are pitted into an arena each year to kill each other? I was expecting it because I've seen Battle Royale, but I would have appreciated knowing more of the backstory of how the world could have come into such a odd state. \\n I suppose what makes a book like this interesting is thinking about the strategy of it all. The players are going to be statistically encouraged to band together because they will last longer that way, but by definition of course any partnership will be broken, and the drama of how that unfolds is always interesting and full of friendships broken and betrayal. Each character approached the game in their own way. Some banded together in larger coalitions, some were loners initially and banded together later. And some were just loners, like Foxface. A lot depended on your survival skill: could you find food and water on your own? Self-dependence is highly valued - and of course our hero was strong there. \\n All in all, a fun read, but I feel kind of dirty for having read it. Wed Jan 13 13:38:25 -0800 2010 Wed Mar 22 11:46:36 -0700 2017       24          25\n",
      "  136251 8842281e1d1347389f2ab93d60773d4d 132eab4c9a3724493204cc083e0e2ecc       5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Loved every minute. So sad there isn't another! \\n I thought JK really made Harry an even stronger archetypal hero - almost in a Paul Maud'Dib from Dune kind of way. He's fighting the ultimate evil, he's brave and takes risks, and believes in himself and doesn't give up despite many hardships. I think JK really did a phenomenal job, as I bet every kid who reads this will have a little bit of hero/Gryffindor in them. \\n The epilogue was lame though. I would have rather seen a '3 months later' epilogue than a '19 years later' one. Mon Dec 22 10:38:27 -0800 2008 Wed Mar 22 11:46:18 -0700 2017        6           3\n",
      "15507958 7504b2aee1ecb5b2872d3da381c6c91e 63ff74279e46b247cb1754313b160006       4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       I finished reading this days ago and cant get this out of my head. It is heart wrenching to see myself in a situation like this. Wed Sep 10 19:33:44 -0700 2014 Sun Sep 21 19:03:38 -0700 2014        0           0\n",
      "\n",
      "========================================================================================================================\n",
      "File Size Comparison:\n",
      "  JSON: 12.08 MB\n",
      "  CSV:  25.12 MB\n",
      "  Saved: -13.04 MB (-107.9%)\n"
     ]
    }
   ],
   "source": [
    "# Convert JSON to CSV\n",
    "print(\"Converting books_1250_above_reviews.json to CSV format...\\n\")\n",
    "\n",
    "# Create DataFrame from reviews\n",
    "df_reviews = pd.DataFrame(reviews_1250_above)\n",
    "\n",
    "# Select key columns for CSV\n",
    "columns_to_keep = ['book_id', 'user_id', 'review_id', 'rating', 'review_text', 'date_added', 'date_updated', 'n_votes', 'n_comments']\n",
    "\n",
    "# Keep only available columns\n",
    "available_cols = [col for col in columns_to_keep if col in df_reviews.columns]\n",
    "df_reviews_csv = df_reviews[available_cols]\n",
    "\n",
    "# Save to CSV\n",
    "csv_output_file = './data/books_1250_above_reviews.csv'\n",
    "df_reviews_csv.to_csv(csv_output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Successfully converted to CSV!\")\n",
    "print(f\"File: {csv_output_file}\")\n",
    "print(f\"Total rows: {len(df_reviews_csv):,}\")\n",
    "print(f\"Columns: {', '.join(available_cols)}\\n\")\n",
    "\n",
    "# Display sample\n",
    "print(\"=\" * 120)\n",
    "print(\"Sample of CSV data (first 5 rows):\\n\")\n",
    "print(df_reviews_csv.head().to_string(index=False))\n",
    "\n",
    "# File size comparison\n",
    "import os\n",
    "json_size = os.path.getsize('./data/books_1250_above_reviews.json') / (1024*1024)\n",
    "csv_size = os.path.getsize(csv_output_file) / (1024*1024)\n",
    "\n",
    "print(f\"\\n{'=' * 120}\")\n",
    "print(f\"File Size Comparison:\")\n",
    "print(f\"  JSON: {json_size:.2f} MB\")\n",
    "print(f\"  CSV:  {csv_size:.2f} MB\")\n",
    "print(f\"  Saved: {(json_size - csv_size):.2f} MB ({((json_size - csv_size) / json_size * 100):.1f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

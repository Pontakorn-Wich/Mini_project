{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c12f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating counts for column 'rating':\n",
      "rating\n",
      "1     493\n",
      "2     908\n",
      "3    2478\n",
      "4    4663\n",
      "5    6073\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# New cell at index 0\n",
    "\n",
    "path = r\"../data/books_1250_above_reviews.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Try common column names first, then try to auto-detect a 1-5 numeric column.\n",
    "common = {\"rating\", \"review_rating\", \"stars\", \"review_stars\", \"score\"}\n",
    "col = next((c for c in df.columns if c.lower() in common), None)\n",
    "\n",
    "if col is None:\n",
    "    for c in df.columns:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\").dropna()\n",
    "        if not s.empty and s.isin([1, 2, 3, 4, 5]).all():\n",
    "            col = c\n",
    "            break\n",
    "\n",
    "if col is None:\n",
    "    raise ValueError(f\"Couldn't find a 1-5 rating column. Available columns: {list(df.columns)}\")\n",
    "\n",
    "counts = pd.to_numeric(df[col], errors=\"coerce\").dropna().astype(int).value_counts().reindex(range(1, 6), fill_value=0).sort_index()\n",
    "print(f\"Rating counts for column '{col}':\\n{counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c749f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence_transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence_transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence_transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence_transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence_transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=1.11.0->sentence_transformers) (77.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kenpo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Using cached sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-5.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\kenpo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0773d",
   "metadata": {},
   "source": [
    "## mixed books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "097a494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "SENTIMENT CLASSIFICATION WITH SENTENCE-BERT\n",
      "========================================================================================================================\n",
      "\n",
      "1. Loading and preparing data...\n",
      "   Total reviews: 14,974\n",
      "   Rating distribution:\n",
      "rating\n",
      "1     493\n",
      "2     908\n",
      "3    2478\n",
      "4    4663\n",
      "5    6073\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Reviews: 14,974\n",
      "   Labels: 14,974\n",
      "\n",
      "2. Splitting data into 80/20 train/test...\n",
      "   Training set: 11,979 reviews\n",
      "   Test set: 2,995 reviews\n",
      "   Training label distribution:\n",
      "0     287\n",
      "1     395\n",
      "2     727\n",
      "3    1982\n",
      "4    3730\n",
      "5    4858\n",
      "Name: count, dtype: int64\n",
      "   Test label distribution:\n",
      "0      72\n",
      "1      98\n",
      "2     181\n",
      "3     496\n",
      "4     933\n",
      "5    1215\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. Loading Sentence-BERT model...\n",
      "   Model: all-MiniLM-L6-v2 (lightweight, fast, good performance)\n",
      "   ✓ Model loaded successfully\n",
      "\n",
      "4. Generating embeddings for training set...\n",
      "   Processing 11,979 reviews...\n",
      "   ✓ Model loaded successfully\n",
      "\n",
      "4. Generating embeddings for training set...\n",
      "   Processing 11,979 reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 375/375 [01:43<00:00,  3.62it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Embedding shape: (11979, 384)\n",
      "   ✓ Embedding dimension: 384\n",
      "\n",
      "5. Generating embeddings for test set...\n",
      "   Processing 2,995 reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 94/94 [00:26<00:00,  3.61it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Test embeddings shape: (2995, 384)\n",
      "\n",
      "6. Training Random Forest classifier on SBERT embeddings...\n",
      "   ✓ Model trained successfully\n",
      "\n",
      "7. Making predictions on test set...\n",
      "   ✓ Predictions completed\n",
      "\n",
      "========================================================================================================================\n",
      "RESULTS\n",
      "========================================================================================================================\n",
      "\n",
      "Accuracy: 0.4898 (48.98%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1★       0.00      0.00      0.00        98\n",
      "          2★       0.50      0.01      0.01       181\n",
      "          3★       0.45      0.17      0.25       496\n",
      "          4★       0.38      0.40      0.39       933\n",
      "          5★       0.55      0.83      0.67      1215\n",
      "\n",
      "   micro avg       0.49      0.50      0.50      2923\n",
      "   macro avg       0.38      0.28      0.26      2923\n",
      "weighted avg       0.46      0.50      0.44      2923\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0    0   21   37   40]\n",
      " [   0    1   39   90   50]\n",
      " [   0    1   85  262  148]\n",
      " [   0    0   38  369  526]\n",
      " [   0    0    4  198 1012]]\n",
      "\n",
      "========================================================================================================================\n",
      "   ✓ Model trained successfully\n",
      "\n",
      "7. Making predictions on test set...\n",
      "   ✓ Predictions completed\n",
      "\n",
      "========================================================================================================================\n",
      "RESULTS\n",
      "========================================================================================================================\n",
      "\n",
      "Accuracy: 0.4898 (48.98%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1★       0.00      0.00      0.00        98\n",
      "          2★       0.50      0.01      0.01       181\n",
      "          3★       0.45      0.17      0.25       496\n",
      "          4★       0.38      0.40      0.39       933\n",
      "          5★       0.55      0.83      0.67      1215\n",
      "\n",
      "   micro avg       0.49      0.50      0.50      2923\n",
      "   macro avg       0.38      0.28      0.26      2923\n",
      "weighted avg       0.46      0.50      0.44      2923\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0    0   21   37   40]\n",
      " [   0    1   39   90   50]\n",
      " [   0    1   85  262  148]\n",
      " [   0    0   38  369  526]\n",
      " [   0    0    4  198 1012]]\n",
      "\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Classification using Sentence-BERT (SBERT)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"SENTIMENT CLASSIFICATION WITH SENTENCE-BERT\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Prepare data\n",
    "print(\"\\n1. Loading and preparing data...\")\n",
    "print(f\"   Total reviews: {len(df):,}\")\n",
    "print(f\"   Rating distribution:\\n{counts}\\n\")\n",
    "\n",
    "# Use review_text and rating\n",
    "X = df['review_text'].values\n",
    "y = pd.to_numeric(df['rating'], errors='coerce').dropna().astype(int).values\n",
    "\n",
    "# Handle any mismatched lengths\n",
    "if len(X) != len(y):\n",
    "    valid_idx = pd.to_numeric(df['rating'], errors='coerce').notna()\n",
    "    X = df[valid_idx]['review_text'].values\n",
    "    y = pd.to_numeric(df[valid_idx]['rating'], errors='coerce').astype(int).values\n",
    "\n",
    "print(f\"   Reviews: {len(X):,}\")\n",
    "print(f\"   Labels: {len(y):,}\")\n",
    "\n",
    "# Split into 80/20 train/test\n",
    "print(\"\\n2. Splitting data into 80/20 train/test...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"   Training set: {len(X_train):,} reviews\")\n",
    "print(f\"   Test set: {len(X_test):,} reviews\")\n",
    "print(f\"   Training label distribution:\\n{pd.Series(y_train).value_counts().sort_index()}\")\n",
    "print(f\"   Test label distribution:\\n{pd.Series(y_test).value_counts().sort_index()}\\n\")\n",
    "\n",
    "# Load SBERT model\n",
    "print(\"3. Loading Sentence-BERT model...\")\n",
    "print(\"   Model: all-MiniLM-L6-v2 (lightweight, fast, good performance)\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"   ✓ Model loaded successfully\\n\")\n",
    "\n",
    "# Generate embeddings for training set\n",
    "print(\"4. Generating embeddings for training set...\")\n",
    "print(f\"   Processing {len(X_train):,} reviews...\")\n",
    "embeddings_train = model.encode(X_train, show_progress_bar=True, batch_size=32)\n",
    "print(f\"   ✓ Embedding shape: {embeddings_train.shape}\")\n",
    "print(f\"   ✓ Embedding dimension: {embeddings_train.shape[1]}\\n\")\n",
    "\n",
    "# Generate embeddings for test set\n",
    "print(\"5. Generating embeddings for test set...\")\n",
    "print(f\"   Processing {len(X_test):,} reviews...\")\n",
    "embeddings_test = model.encode(X_test, show_progress_bar=True, batch_size=32)\n",
    "print(f\"   ✓ Test embeddings shape: {embeddings_test.shape}\\n\")\n",
    "\n",
    "# Train classifier on embeddings\n",
    "print(\"6. Training Random Forest classifier on SBERT embeddings...\")\n",
    "classifier = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)\n",
    "classifier.fit(embeddings_train, y_train)\n",
    "print(\"   ✓ Model trained successfully\\n\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"7. Making predictions on test set...\")\n",
    "y_pred = classifier.predict(embeddings_test)\n",
    "print(\"   ✓ Predictions completed\\n\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, labels=[1, 2, 3, 4, 5], target_names=['1★', '2★', '3★', '4★', '5★']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Store results for visualization\n",
    "results = {\n",
    "    'model': classifier,\n",
    "    'embeddings_train': embeddings_train,\n",
    "    'embeddings_test': embeddings_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'y_pred': y_pred,\n",
    "    'accuracy': accuracy,\n",
    "    'cm': cm\n",
    "}\n",
    "\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e1f1e",
   "metadata": {},
   "source": [
    "## separated books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2bdef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "SENTIMENT CLASSIFICATION BY INDIVIDUAL BOOK (80/20 Train/Test Split)\n",
      "========================================================================================================================\n",
      "\n",
      "[Book 1/8] Processing book_id: 7905092\n",
      "   Total reviews: 1,299\n",
      "   Valid reviews: 1,299\n",
      "   Rating distribution: {0: np.int64(54), 1: np.int64(107), 2: np.int64(145), 3: np.int64(285), 4: np.int64(392), 5: np.int64(316)}\n",
      "   Train: 1,039 | Test: 260\n",
      "   ✓ Accuracy: 0.3500 (35.00%)\n",
      "\n",
      "[Book 2/8] Processing book_id: 10429045\n",
      "   Total reviews: 3,158\n",
      "   Valid reviews: 3,158\n",
      "   Rating distribution: {0: np.int64(82), 1: np.int64(208), 2: np.int64(256), 3: np.int64(585), 4: np.int64(970), 5: np.int64(1057)}\n",
      "   Train: 2,526 | Test: 632\n",
      "   ✓ Accuracy: 0.3500 (35.00%)\n",
      "\n",
      "[Book 2/8] Processing book_id: 10429045\n",
      "   Total reviews: 3,158\n",
      "   Valid reviews: 3,158\n",
      "   Rating distribution: {0: np.int64(82), 1: np.int64(208), 2: np.int64(256), 3: np.int64(585), 4: np.int64(970), 5: np.int64(1057)}\n",
      "   Train: 2,526 | Test: 632\n",
      "   ✓ Accuracy: 0.4146 (41.46%)\n",
      "\n",
      "[Book 3/8] Processing book_id: 12609433\n",
      "   Total reviews: 1,173\n",
      "   Valid reviews: 1,173\n",
      "   Rating distribution: {0: np.int64(26), 1: np.int64(9), 2: np.int64(51), 3: np.int64(213), 4: np.int64(489), 5: np.int64(385)}\n",
      "   Train: 938 | Test: 235\n",
      "   ✓ Accuracy: 0.4146 (41.46%)\n",
      "\n",
      "[Book 3/8] Processing book_id: 12609433\n",
      "   Total reviews: 1,173\n",
      "   Valid reviews: 1,173\n",
      "   Rating distribution: {0: np.int64(26), 1: np.int64(9), 2: np.int64(51), 3: np.int64(213), 4: np.int64(489), 5: np.int64(385)}\n",
      "   Train: 938 | Test: 235\n",
      "   ✓ Accuracy: 0.4766 (47.66%)\n",
      "\n",
      "[Book 4/8] Processing book_id: 13227454\n",
      "   Total reviews: 1,589\n",
      "   Valid reviews: 1,589\n",
      "   Rating distribution: {0: np.int64(35), 1: np.int64(23), 2: np.int64(77), 3: np.int64(266), 4: np.int64(640), 5: np.int64(548)}\n",
      "   Train: 1,271 | Test: 318\n",
      "   ✓ Accuracy: 0.4766 (47.66%)\n",
      "\n",
      "[Book 4/8] Processing book_id: 13227454\n",
      "   Total reviews: 1,589\n",
      "   Valid reviews: 1,589\n",
      "   Rating distribution: {0: np.int64(35), 1: np.int64(23), 2: np.int64(77), 3: np.int64(266), 4: np.int64(640), 5: np.int64(548)}\n",
      "   Train: 1,271 | Test: 318\n",
      "   ✓ Accuracy: 0.4371 (43.71%)\n",
      "\n",
      "[Book 5/8] Processing book_id: 13455782\n",
      "   Total reviews: 943\n",
      "   Valid reviews: 943\n",
      "   Rating distribution: {0: np.int64(18), 1: np.int64(36), 2: np.int64(90), 3: np.int64(200), 4: np.int64(319), 5: np.int64(280)}\n",
      "   Train: 754 | Test: 189\n",
      "   ✓ Accuracy: 0.4371 (43.71%)\n",
      "\n",
      "[Book 5/8] Processing book_id: 13455782\n",
      "   Total reviews: 943\n",
      "   Valid reviews: 943\n",
      "   Rating distribution: {0: np.int64(18), 1: np.int64(36), 2: np.int64(90), 3: np.int64(200), 4: np.int64(319), 5: np.int64(280)}\n",
      "   Train: 754 | Test: 189\n",
      "   ✓ Accuracy: 0.4339 (43.39%)\n",
      "\n",
      "[Book 6/8] Processing book_id: 18774964\n",
      "   Total reviews: 3,809\n",
      "   Valid reviews: 3,809\n",
      "   Rating distribution: {0: np.int64(76), 1: np.int64(29), 2: np.int64(81), 3: np.int64(318), 4: np.int64(992), 5: np.int64(2313)}\n",
      "   Train: 3,047 | Test: 762\n",
      "   ✓ Accuracy: 0.4339 (43.39%)\n",
      "\n",
      "[Book 6/8] Processing book_id: 18774964\n",
      "   Total reviews: 3,809\n",
      "   Valid reviews: 3,809\n",
      "   Rating distribution: {0: np.int64(76), 1: np.int64(29), 2: np.int64(81), 3: np.int64(318), 4: np.int64(992), 5: np.int64(2313)}\n",
      "   Train: 3,047 | Test: 762\n",
      "   ✓ Accuracy: 0.6142 (61.42%)\n",
      "\n",
      "[Book 7/8] Processing book_id: 22738563\n",
      "   Total reviews: 1,526\n",
      "   Valid reviews: 1,526\n",
      "   Rating distribution: {0: np.int64(27), 1: np.int64(8), 2: np.int64(17), 3: np.int64(119), 4: np.int64(366), 5: np.int64(989)}\n",
      "   Train: 1,220 | Test: 306\n",
      "   ✓ Accuracy: 0.6142 (61.42%)\n",
      "\n",
      "[Book 7/8] Processing book_id: 22738563\n",
      "   Total reviews: 1,526\n",
      "   Valid reviews: 1,526\n",
      "   Rating distribution: {0: np.int64(27), 1: np.int64(8), 2: np.int64(17), 3: np.int64(119), 4: np.int64(366), 5: np.int64(989)}\n",
      "   Train: 1,220 | Test: 306\n",
      "   ✓ Accuracy: 0.6569 (65.69%)\n",
      "\n",
      "[Book 8/8] Processing book_id: 25781157\n",
      "   Total reviews: 1,477\n",
      "   Valid reviews: 1,477\n",
      "   Rating distribution: {0: np.int64(41), 1: np.int64(73), 2: np.int64(191), 3: np.int64(492), 4: np.int64(495), 5: np.int64(185)}\n",
      "   Train: 1,181 | Test: 296\n",
      "   ✓ Accuracy: 0.6569 (65.69%)\n",
      "\n",
      "[Book 8/8] Processing book_id: 25781157\n",
      "   Total reviews: 1,477\n",
      "   Valid reviews: 1,477\n",
      "   Rating distribution: {0: np.int64(41), 1: np.int64(73), 2: np.int64(191), 3: np.int64(492), 4: np.int64(495), 5: np.int64(185)}\n",
      "   Train: 1,181 | Test: 296\n",
      "   ✓ Accuracy: 0.4291 (42.91%)\n",
      "\n",
      "========================================================================================================================\n",
      "SUMMARY BY BOOK\n",
      "========================================================================================================================\n",
      " Book ID  Total Reviews  Train Set  Test Set        Accuracy\n",
      " 7905092           1299       1039       260 0.3500 (35.00%)\n",
      "10429045           3158       2526       632 0.4146 (41.46%)\n",
      "12609433           1173        938       235 0.4766 (47.66%)\n",
      "13227454           1589       1271       318 0.4371 (43.71%)\n",
      "13455782            943        754       189 0.4339 (43.39%)\n",
      "18774964           3809       3047       762 0.6142 (61.42%)\n",
      "22738563           1526       1220       306 0.6569 (65.69%)\n",
      "25781157           1477       1181       296 0.4291 (42.91%)\n",
      "\n",
      "Average Accuracy: 0.4765 (47.65%)\n",
      "========================================================================================================================\n",
      "   ✓ Accuracy: 0.4291 (42.91%)\n",
      "\n",
      "========================================================================================================================\n",
      "SUMMARY BY BOOK\n",
      "========================================================================================================================\n",
      " Book ID  Total Reviews  Train Set  Test Set        Accuracy\n",
      " 7905092           1299       1039       260 0.3500 (35.00%)\n",
      "10429045           3158       2526       632 0.4146 (41.46%)\n",
      "12609433           1173        938       235 0.4766 (47.66%)\n",
      "13227454           1589       1271       318 0.4371 (43.71%)\n",
      "13455782            943        754       189 0.4339 (43.39%)\n",
      "18774964           3809       3047       762 0.6142 (61.42%)\n",
      "22738563           1526       1220       306 0.6569 (65.69%)\n",
      "25781157           1477       1181       296 0.4291 (42.91%)\n",
      "\n",
      "Average Accuracy: 0.4765 (47.65%)\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Classification for Individual Books\n",
    "print(\"=\" * 120)\n",
    "print(\"SENTIMENT CLASSIFICATION BY INDIVIDUAL BOOK (80/20 Train/Test Split)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Group reviews by book_id\n",
    "book_groups = df.groupby('book_id')\n",
    "\n",
    "book_results = {}\n",
    "\n",
    "for idx, (book_id, book_df) in enumerate(book_groups, 1):\n",
    "    print(f\"\\n[Book {idx}/8] Processing book_id: {book_id}\")\n",
    "    print(f\"   Total reviews: {len(book_df):,}\")\n",
    "    \n",
    "    # Get reviews and ratings for this book\n",
    "    X_book = book_df['review_text'].values\n",
    "    y_book = pd.to_numeric(book_df['rating'], errors='coerce').dropna().astype(int).values\n",
    "    \n",
    "    # Handle mismatched lengths\n",
    "    if len(X_book) != len(y_book):\n",
    "        valid_idx = pd.to_numeric(book_df['rating'], errors='coerce').notna()\n",
    "        X_book = book_df[valid_idx]['review_text'].values\n",
    "        y_book = pd.to_numeric(book_df[valid_idx]['rating'], errors='coerce').astype(int).values\n",
    "    \n",
    "    print(f\"   Valid reviews: {len(X_book):,}\")\n",
    "    print(f\"   Rating distribution: {dict(pd.Series(y_book).value_counts().sort_index())}\")\n",
    "    \n",
    "    # Split data (80/20 with stratification)\n",
    "    try:\n",
    "        X_train_book, X_test_book, y_train_book, y_test_book = train_test_split(\n",
    "            X_book, y_book, test_size=0.2, random_state=42, stratify=y_book\n",
    "        )\n",
    "        print(f\"   Train: {len(X_train_book):,} | Test: {len(X_test_book):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Skipping book (split error): {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate embeddings\n",
    "    try:\n",
    "        embeddings_train_book = model.encode(X_train_book, show_progress_bar=False, batch_size=32)\n",
    "        embeddings_test_book = model.encode(X_test_book, show_progress_bar=False, batch_size=32)\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Skipping book (embedding error): {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Train classifier\n",
    "    try:\n",
    "        classifier_book = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)\n",
    "        classifier_book.fit(embeddings_train_book, y_train_book)\n",
    "        y_pred_book = classifier_book.predict(embeddings_test_book)\n",
    "        accuracy_book = accuracy_score(y_test_book, y_pred_book)\n",
    "        \n",
    "        print(f\"   ✓ Accuracy: {accuracy_book:.4f} ({accuracy_book*100:.2f}%)\")\n",
    "        \n",
    "        # Store results\n",
    "        book_results[book_id] = {\n",
    "            'n_reviews': len(X_book),\n",
    "            'n_train': len(X_train_book),\n",
    "            'n_test': len(X_test_book),\n",
    "            'accuracy': accuracy_book,\n",
    "            'classifier': classifier_book,\n",
    "            'y_test': y_test_book,\n",
    "            'y_pred': y_pred_book,\n",
    "            'embeddings_train': embeddings_train_book,\n",
    "            'embeddings_test': embeddings_test_book\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Skipping book (training error): {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"SUMMARY BY BOOK\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "if book_results:\n",
    "    summary_df = pd.DataFrame([\n",
    "        {\n",
    "            'Book ID': book_id,\n",
    "            'Total Reviews': result['n_reviews'],\n",
    "            'Train Set': result['n_train'],\n",
    "            'Test Set': result['n_test'],\n",
    "            'Accuracy': f\"{result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\"\n",
    "        }\n",
    "        for book_id, result in book_results.items()\n",
    "    ])\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    avg_accuracy = np.mean([r['accuracy'] for r in book_results.values()])\n",
    "    print(f\"\\nAverage Accuracy: {avg_accuracy:.4f} ({avg_accuracy*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"No results available.\")\n",
    "\n",
    "print(\"=\" * 120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
